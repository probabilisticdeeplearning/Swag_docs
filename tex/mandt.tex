\newcommand\batchSize{B}
\newcommand\sqrtC{C^{1/2}}
\newcommand\loss{\mathcal{L}}
\newcommand\lr{\eta}
\newcommand\yBar{\overline{y}}
\newcommand\yBarB{\overline{y}_{\mathcal{B}}}
\newcommand\yBarNotB{\overline{y}_{\mathcal{N} \setminus \mathcal{B}}}



\section{Mandt - Continuous}

\begin{align}
  \loss(\theta) &= \frac{1}{N} \sum_{n=1}^N l_n(\theta) \nonumber \\
  l_n(\theta) &= - \log p(x_n | \theta) - \frac{1}{N} \log p(\theta) \nonumber \\
  g(\theta) &= \nabla_{\theta} \loss(\theta)
  \label{eq:loss}
\end{align}

\begin{align}
  \hat{g}_S &\approx g(\theta) + \frac{1}{\sqrt{\batchSize}} \Delta g(\theta),\ \Delta g(\theta) \sim N(0, C(\theta)) \nonumber \\
  C(\theta) &\approx C = B B^T
  \label{eq:grad_noise}
\end{align}

\begin{align}
  \theta(t + 1) = \theta(t) - \lr \hat{g}_S( \theta(t) )
  \label{eq:sgd_step}
\end{align}

Eq's \eqref{eq:grad_noise}, \eqref{eq:sgd_step} gives the process:

\begin{align}
  \Delta \theta(t) = - \lr g( \theta(t) ) + \frac{\lr}{\batchSize} \sqrtC \Delta W,\ \Delta W \sim \mathcal{N}(0, I)
  \label{eq:sgd_step}
\end{align}

\begin{align}
  d \theta(t) = - \lr g( \theta )dt + \frac{\lr}{\batchSize} \sqrtC  dW(t)
  \label{eq:continuous:sgd_step}
\end{align}

\section{Linear}

\begin{align}
  \theta &\in \mathbb{R}^d \nonumber \\
  x_n &\in \mathbb{R}^d \nonumber \\
  y_n &\in \mathbb{R} \nonumber \\
  X &= (x_1, \dots, x_N)\in \mathbb{R}^{d \times N} \nonumber \\
  Y &= (y_1, \dots, y_N)^T \in \mathbb{R}^{N} \nonumber \\
\end{align}

\begin{align}
  \loss(\theta) &= \frac{1}{2} \sum_{n=1}^N  \| x_n^T \theta - y_n \|^2 = \frac{1}{2} \| X^T \theta - Y \|^2 \nonumber \\
  g(\theta) &= X (X^T \theta - Y)
  \label{eq:linear_loss}
\end{align}

\begin{align}
  \Delta \theta(t) = - \lr X (X^T \theta - Y) + \frac{\lr}{\batchSize} \sqrtC \Delta W
  \label{eq:sgd_diff}
\end{align}

How to relate it to the posterior?

\section{Scalar}

Notation:
\begin{align}
  \mathcal{N} = \{1, \dots, N\} \nonumber \\
  \mathcal{B} \subset \mathcal{N},\ |\mathcal{B}| = B
\end{align}

\begin{align}
  \theta &\in \mathbb{R} \nonumber \\
  x_n &\in \mathbb{R} \nonumber \\
  y_n &\in \mathbb{R} \nonumber \\
\end{align}

\begin{align}
  \loss(\theta) &= \frac{1}{2N} \sum_{n \in \mathcal{N}} (\theta - y_n)^2 \nonumber \\
  \Rightarrow g_n(\theta) &= \theta - y_n \nonumber \\
  \Rightarrow \hat{g}_B(\theta) &= \theta - \yBar \nonumber \\
  \label{eq:linear_loss}
\end{align}

\begin{align}
  \theta_{k+1} = \theta_k - \lr (\theta_k - \yBarB) = (1- \lr) \theta_k + \lr \yBarB
  \label{eq:sgd_step_scalar}
\end{align}

\subsection{$p(\yBarB | \yBar )$}

\begin{equation}
  \yBar = \frac{B}{N} \yBarB + \frac{N-B}{N} \yBarNotB
\end{equation}

Known:

\begin{align}
  p(\yBarB, \yBarNotB | \theta ) &= p(\yBarB | \theta) p(\yBarNotB | \theta ) \nonumber \\
  p(\yBarB | \theta) &= \mathcal{N}(\yBarB; \theta, \sigma_B) \nonumber \\
  p(\yBarNotB | \theta) &= \mathcal{N}(\yBarNotB; \theta, \sigma_{N-B}) \nonumber \\
\end{align}
Note, is the first one really independent?

Sought:

\begin{align}
  p(\yBarB | \theta, \yBar ) &= \frac{p(\yBar, \yBarB | \theta)}{p(\yBar | \theta)}\nonumber \\
                             &= \frac{p(\yBar | \yBarB, \theta) p(\yBarB | \theta) }{p(\yBar | \theta)} \nonumber \\
  \label{eq:pyb_y}
\end{align}
Need:

\begin{align}
  p(\yBar | \yBarB, \theta) "=" p(\yBarNotB
  = f(\yBarB, \yBar)| \theta)
  = p_{\yBarNotB| \theta} (f(\yBarB, \yBar)| \theta),
  \label{eq:py_yb_theta}
\end{align}
where,
\begin{equation}
  \yBarNotB = \frac{N}{N-B}(\yBar - \frac{B}{N} \yBarB).
\end{equation}

Since we condition on $\yBarB$ we know a deterministic transformation from $\yBar$ to $\yBarNotB$.
Which essentialy gives us a distribution over a shifted variable.

Written out with actual normal distributions, the limits converge to what we expect

TODO: transfer hand written notes

