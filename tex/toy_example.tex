\section{Toy example}

\subsection{Data}

Consider simulated data, with $n$ normally distributed observations with mean parameter $\bm{\theta}$ from a similarly normal prior with known variance:

\begin{align}
  \bm{\theta} &\sim \mathcal{N}(\bm{\theta}; \bm{\theta}_0, \Sigma_{\bm{\theta}}), \nonumber \\
  \bm{x} | \bm{\theta} &\sim \mathcal{N}(\bm{x}; \bm{\theta}, \Sigma_{\bm{x}}).
\end{align}

Which gives a likelhood function of the sampled values:

\begin{align}
  \{\bm{x}\}_{n=1}^N | \bm{\theta} \sim \prod_{n=1}^N f(\bm{x}; \bm{\theta}, \Sigma_{\bm{x}}),
\end{align}
where

\begin{equation}
  f \left( \bm{x}; \bm{\mu}, \Sigma \right) =
  \frac{ \exp \left( -\frac{ 1 }{ 2 } ( \bm {x} - \bm{\mu} )^{ \mathrm {T} }{ \Sigma }^{-1} ( \bm{x} - \bm{\mu} ) \right)} { \sqrt{ ( 2\pi )^{k} | \Sigma | } }.
\end{equation}

is a normal pdf and the corresponding negative log.\ likelihood:

\begin{align}
  -\log f \left( \bm{x}; \bm{\mu}, \Sigma \right) = \sum_{n=1}^N \frac{ 1 }{ 2 } ( \bm {x} - \bm{\mu} )^{ \mathrm {T} }{ \Sigma }^{-1} ( \bm{x} - \bm{\mu} ) + \frac{k}{2} \log 2\pi  + \frac{1}{2} \log | \Sigma | .
\end{align}
Under these assumptions the likelihood and prior conjugate and the posterior distribution is also normal:

\begin{align}
  \bm{\theta}|\bm{x} &\sim \mathcal{N}(\bm{\theta}; \bm{\theta}', \Sigma_{\bm{\theta}}') \nonumber,
\end{align}
where

\begin{align}
  \bm{\theta}' &= \left( \Sigma_{\bm{\theta}}^{-1} + n \Sigma_x^{-1}\right)^{-1} \left( \Sigma_{\bm{\theta}}^{-1} \bm{\theta}_{0} + \Sigma_x^{-1} \sum_{i=1}^{n} \bm{x}_{i} \right) \nonumber \\
  \Sigma_{\bm{\theta}}' &=\left( \Sigma_{\bm{\theta}}^{-1} + n \Sigma_x^{-1}\right)^{-1}
  \label{eq:posterior_update}
\end{align}
A sample $\{\bm{x}_1, \dots, \bm{x}_B \}$ with batch size $B$ is created by first sampling a $\theta$ from the prior and then use that $\theta$ when sampling from $ \bm{x} \sim \theta $.
Then the true posterior will be determined by updating the prior (eq \eqref{eq:posterior_update}) after each batch and consider the last posterior as the current prior.

Perhaps I should let $\Sigma_x$ be a parameter as well? But if we want a normal posterior then it needs to be known.

\subsection{Learning}

I base my loss function on the posterior, like they do in\ \cite{swag}. In practice I sum the negative logarithm of the likelihood and the posterior:

\begin{align}
  \mathcal{L} &=
    \left( \displaystyle{\sum_{i=1}^n} - \log p( \bm{x}_i | \bm{\theta} ) \right)
    - \log p( \bm{\theta} ) \propto \log p( \bm{\theta} | \bm{x} )
    \nonumber \\
  \therefore \mathcal{L} &=
    \left( \displaystyle{\sum_{i=1}^n} - \log f( \bm{x}_i; \bm{\theta}, \Sigma_{\bm{x}} ) \right).
    - \log f( \bm{\theta}; \bm{\theta}_0, \Sigma_{\bm{\theta}} )
\end{align}
and use the \texttt{pytorch} implementations of the distributions to calculate the log probability.
But for this example this can be further simplified:

\begin{align}
  \mathcal{L} &= 
              \sum_{n=1}^N \frac{ 1 }{ 2 } ( \bm {x}_i - \bm{\theta} )^{ \mathrm {T} }\Sigma_{\bm{x}}^{-1} ( \bm{x} - \bm{\theta} ) + \frac{k}{2} \log 2\pi  + \frac{1}{2} \log | \Sigma_{\bm{x}} | + \nonumber \\
              &\frac{ 1 }{ 2 } ( \bm {\theta} - \bm{\theta}_0 )^{ \mathrm {T} }\Sigma_{\bm{\theta}}^{-1} ( \bm{\theta} - \bm{\theta}_0 ) + \frac{k}{2} \log 2\pi  + \frac{1}{2} \log | \Sigma_{\bm{\theta}} | \nonumber \\
              &\propto_{\bm{\theta}}
              \left( \sum_{n=1}^N( \bm {x}_i - \bm{\theta} )^{ \mathrm {T} }\Sigma_{\bm{x}}^{-1} ( \bm{x}_i - \bm{\theta} ) \right) +
              ( \bm {\theta} - \bm{\theta}_0 )^{ \mathrm {T} }\Sigma_{\bm{\theta}}^{-1} ( \bm{\theta} - \bm{\theta}_0 ) + C
\end{align}
The code for algorithm~\ref{alg:swag_gaussian} is in the
\href{https://github.com/probabilisticdeeplearning/swa_gaussian/tree/toy_example}{\texttt{repo}} (branch \texttt{toy\_example})

\begin{algorithm}
  \SetKwInOut{Input}{Input}
  \SetKwInOut{Output}{Output}
  \SetKw{KwBy}{by}

  \Input{$\bm{\theta}_0, \Sigma_{\bm{\theta}}, \Sigma_{\bm{x}}$}
  \Output{$\{\bm{\hat{\theta}}\}$}
  $\{\bm{\hat{\theta}}\} = \emptyset$\\
  \For{epoch in $1, \dots, T$}
  {
    \For{batch in $1, \dots, n/B$}
    {
      $l = 0$ \\
      \For{i in $1, \dots, B$}
      {
        sample $\bm{\theta} \sim \bm{\theta} | \bm{\theta}_0$ ?\\
        sample $\bm{x}_i \sim \bm{x}_i | \bm{\theta}$\\
        $l \to l + \mathcal{L}(\bm{x}_i, \bm{\hat{\theta}})$\\
      }
      $\bm{\hat{\theta}} \to sgd(\bm{\hat{\theta}}, l)$ \\
      $\{\bm{\hat{\theta}}\} \to \{\bm{\hat{\theta}}\} \cup \bm{\hat{\theta}}$ \\
    }
  }
  \caption{Swag, Normal likelihood, normal prior}\label{alg:swag_gaussian}
\end{algorithm}

\input{tex/results.tex}
